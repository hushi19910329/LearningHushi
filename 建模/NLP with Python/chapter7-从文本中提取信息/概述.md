# 信息提取概述

- 信息提取是什么（在这本书里面）

  我们首先将自然语言的句子转化为结构化的数据，如下表所示：

```
  OrgName             LocationName
  --------------------------------
  Omnicom             New York
  DDB Needham         New York
  Kaplan Thaler Group New York
  BBDO South          Atlanta
  Georgia-Pacific     Atlanta
```
  然后用sql这一类的工具进行查询，查询的结果可能是：

```
[PER: u'Kiriyenko'] u'became president of the' [ORG: u'NORSI']

```

总的来看，这是一个三元组，主语、宾语和中间的关系部分。

## 信息抽取的步骤：

信息抽取分为以下几个步骤：

1. 文本切分，将string类型的文本划分为list类型的句子
2. 句子切分，将每个list类型的句子划分成由单词或chunk组成的list
3. 词性标注，生成由一个list，其组成内容是多个形如(word,lable)的tuple
4. 命名实体识别，生成各个tree组成一个list
5. 关系识别，生成一个list，**其内容是多个形如(entity,relation,entity)的tuple**

执行前面三项，我们可以直接引用nltk中默认的**句子分割器、分词器和词性标注器**，当然，对于中文，我们还有强大的**结巴分词**这个软件可以用。

### nltk
1. 将文本切分为句子`nltk.sent_tokenize(doc)`

```Py
>>> import nltk
>>> text=" Welcome readers. I hope you find it interesting. Please do reply."
>>> from nltk.tokenize import sent_tokenize
>>> print(sent_tokenize(text))
[' Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']
```
2. 将句子切分为单词`nltk.word_tokenize(sentence)`

```py
>>> text=" Hello everyone. Hope all are fine and doing well. Hope you find the book interesting"
>>> nltk.word_tokenize(text)
['Hello',
 'everyone',
 '.',
 'Hope',
 'all',
 'are',
 'fine',
 'and',
 'doing',
 'well',
 '.',
 'Hope',
 'you',
 'find',
 'the',
 'book',
 'interesting']
```

3. 词性标注'nltk.pos_tag(word)'

```py
>>> text=" Hello everyone. Hope all are fine and doing well. Hope you find the book interesting"
>>> [nltk.pos_tag([word]) for word in nltk.word_tokenize(text)]

[[('Hello', 'NN')],
 [('everyone', 'NN')],
 [('.', '.')],
 [('Hope', 'NN')],
 [('all', 'DT')],
 [('are', 'VBP')],
 [('fine', 'NN')],
 [('and', 'CC')],
 [('doing', 'VBG')],
 [('well', 'RB')],
 [('.', '.')],
 [('Hope', 'NN')],
 [('you', 'PRP')],
 [('find', 'VB')],
 [('the', 'DT')],
 [('book', 'NN')],
 [('interesting', 'VBG')]]
```
### 结巴分词

结巴分词库似乎没有专门用于切分句子的模块，只有分词的功能，不过这并不影响使用。

1. 分词`jieba.cut()`
```
Help on method cut in module jieba_fast:

cut(sentence, cut_all=False, HMM=True) method of jieba_fast.Tokenizer instance
    The main function that segments an entire sentence that contains
    Chinese characters into seperated words.

    Parameter:
        - sentence: The str(unicode) to be segmented.
        - cut_all: Model type. True for full pattern, False for accurate pattern.
        - HMM: Whether to use the Hidden Markov Model.
```

2. 词性标注

```py
>>> words = posseg.cut("我爱北京天安门")
>>> for word, flag in words:
>>>     print('%s %s' % (word, flag))

我 r
爱 v
北京 ns
天安门 ns
```

这里可以看到，词性标注已经做了分词的事情了，所以，其实直接使用词性标注的功能应该大部分情况下都够用了。

---

## 总结1

那么，这里基本看了下在做正式的任务之前要做的准备工作，就是：

分句-->分词-->词性标注！

而词性标注实际上已经把前面两步的事情都做好了。

后面要做的主要的就是NER（命名实体识别）和关系抽取。
---

## 命名实体识别

命名实体可以很直观地这样来理解：命名就是进行标注，那么实体呢，就是具有特定类型的个体，比如：人名、地名、设施等等。命名实体就是找到这些实体并且给其打上标签。

命名实体举例就是：('毛主席','Person')，将'毛主席'这样一个实体打上'Person'这个类型的标签的过程。

这个任务人做起来很简单，但是让机器来做就很难了，先不看原理，看看我们已有的工具。

nltk已经提供了一个训练好的识别命名实体的分类器`nltk.ne_chunk()`:

```py
>>> import nltk
>>> sent = nltk.corpus.treebank.tagged_sents()[1]
>>> print(nltk.ne_chunk(sent))

(S
  (PERSON Mr./NNP)
  (PERSON Vinken/NNP)
  is/VBZ
  chairman/NN
  of/IN
  (ORGANIZATION Elsevier/NNP)
  N.V./NNP
  ,/,
  the/DT
  (GPE Dutch/NNP)
  publishing/VBG
  group/NN
  ./.)
```

命名实体大概就是这么一件事情，结果如上，结果中的第一个S大概是表示Start的意思，没有仔细去查证了!每一行就是一个单词和对应的标注，包括词性和实体的类别。有括号的应该是实体，其他的是表示实体之间的关系的。

## 关系抽取

关系抽取的目的就是寻找所有(X,r,Y)这样的三元组，其中，X和Y是命名实体，r是两者之间的关系。如(Trump, in, House) 大概就是表示特朗普在白宫，这个关系就比较清楚了。

那么怎么得到这个关系呢？nltk提供了一种特殊的正则表达式的方法，看个例子：

```py
>>> IN = re.compile(r'.*\bin\b(?!\b.+ing)')
>>> for doc in nltk.corpus.ieer.parsed_docs('NY_19980315'):
>>>   for rel in nltk.sem.extract_rels('ORG','LOC',doc, corpus = 'ieer', pattern = IN)
>>>     print(nltk.sem.show_raw_tuple(rel))

[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']
[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']
[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']
[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']
[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']
[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']
[ORG: 'WGBH'] 'in' [LOC: 'Boston']
[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']
[ORG: 'Omnicom'] 'in' [LOC: 'New York']
[ORG: 'DDB Needham'] 'in' [LOC: 'New York']
[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']
[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']
[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']
```
